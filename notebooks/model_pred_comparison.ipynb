{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import gzip\n",
    "import os\n",
    "import cv2\n",
    "from ipywidgets import interact, interactive, IntSlider, ToggleButtons\n",
    "import torch\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import sys\n",
    "\n",
    "# for relative imports to work in notebooks\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from model.model  import BraTS2021BaseUnetModel, BraTS2021AttentionUnetModel_V4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select which BRaTS2021 case we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_id = '00166'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model is located at: \n",
    "unet_base_model_path        = '../saved/models/base_unet_dice_no_augm/model_best.pth'\n",
    "unet_base_aug_model_path    = '../saved/models/base_unet_dice/model_best.pth'\n",
    "unet_attention_model_path   = '../saved/models/attention_unet_dice_v4_all_aug_30_124/model_best.pth'\n",
    "\n",
    "# load base unet model\n",
    "base_model      = BraTS2021BaseUnetModel()\n",
    "base_aug_model  = BraTS2021BaseUnetModel()\n",
    "att_model       = BraTS2021AttentionUnetModel_V4()\n",
    "\n",
    "# get checkpoints\n",
    "base_model_checkpoint = torch.load(unet_base_model_path)\n",
    "base_aug_model_checkpoint = torch.load(unet_base_aug_model_path)\n",
    "att_model_checkpoint = torch.load(unet_attention_model_path)\n",
    "\n",
    "# get state dicts\n",
    "base_model_state_dict = base_model_checkpoint['state_dict']\n",
    "base_aug_model_state_dict = base_aug_model_checkpoint['state_dict']\n",
    "att_model_state_dict = att_model_checkpoint['state_dict']\n",
    "\n",
    "# load state dicts\n",
    "base_model.load_state_dict(base_model_state_dict)\n",
    "base_aug_model.load_state_dict(base_aug_model_state_dict)\n",
    "att_model.load_state_dict(att_model_state_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all modalities and segmentation groundtruth masks for one case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resolution: 240x240\n",
      "Number of slices: 155\n"
     ]
    }
   ],
   "source": [
    "# define path\n",
    "base_path = '../data/BRaTS2021/BRaTS2021_raw/' \n",
    "\n",
    "Flair       = nib.load(base_path  + 'BraTS2021_' + sample_id  + '/BraTS2021_' + sample_id + '_flair.nii.gz').get_fdata()\n",
    "seg_target  = nib.load(base_path  + 'BraTS2021_' + sample_id  + '/BraTS2021_' + sample_id + '_seg.nii.gz').get_fdata()\n",
    "T1          = nib.load(base_path  + 'BraTS2021_' + sample_id  + '/BraTS2021_' + sample_id + '_t1.nii.gz').get_fdata()\n",
    "T1ce        = nib.load(base_path  + 'BraTS2021_' + sample_id  + '/BraTS2021_' + sample_id + '_t1ce.nii.gz').get_fdata()\n",
    "T2          = nib.load(base_path  + 'BraTS2021_' + sample_id  + '/BraTS2021_' + sample_id + '_t2.nii.gz').get_fdata()\n",
    "\n",
    "# convert from 0, 1, 2, 4 --> 0, 1, 2, 3\n",
    "seg_target[seg_target == 4] = 3  \n",
    "\n",
    "# print number of slices\n",
    "imgshape = Flair.shape\n",
    "print(f\"Image resolution: {imgshape[0]}x{imgshape[1]}\")\n",
    "print(f\"Number of slices: {imgshape[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create transformations for data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformations.transformations import brats_validation_transform\n",
    "\n",
    "brats_transform = brats_validation_transform(image_keys=['t1', 't1ce', 't2', 'flair'], \n",
    "                                             all_keys=['t1', 't1ce', 't2', 'flair', 'seg'],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the specific case we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MRI modalities\n",
    "modalities = ['t1', 't1ce', 't2', 'flair'] \n",
    "\n",
    "# placeholders \n",
    "slice_dict = {}\n",
    "\n",
    "# loop over all 155 slices\n",
    "for slice_id in range(155):\n",
    "    # create dictionary for each slice\n",
    "    slice_dict[slice_id] = {}\n",
    "\n",
    "    # get all 4 modalities\n",
    "    t1 = T1[:, :, slice_id]\n",
    "    t1ce = T1ce[:, :, slice_id]\n",
    "    t2 = T2[:, :, slice_id]\n",
    "    flair = Flair[:, :, slice_id]\n",
    "    seg_tar = seg_target[:, :, slice_id]\n",
    "\n",
    "    # create dictionary for each slice \n",
    "    slice_dict[slice_id]['t1'] = t1\n",
    "    slice_dict[slice_id]['t1ce'] = t1ce\n",
    "    slice_dict[slice_id]['t2'] = t2\n",
    "    slice_dict[slice_id]['flair'] = flair\n",
    "    slice_dict[slice_id]['seg'] = seg_tar\n",
    "\n",
    "    # apply transformations\n",
    "    slice_dict[slice_id] = brats_transform(slice_dict[slice_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the loaded case to get the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of seg_preds_base:  155\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# move all models to GPU\n",
    "base_model.to(device)\n",
    "base_aug_model.to(device)\n",
    "att_model.to(device)\n",
    "\n",
    "# set all models to eval mode\n",
    "base_model.eval()\n",
    "base_aug_model.eval()\n",
    "att_model.eval()\n",
    "\n",
    "seg_preds_base = []\n",
    "seg_preds_base_aug = []\n",
    "seg_preds_att = []\n",
    "\n",
    "# n_cases = 2\n",
    "\n",
    "with torch.no_grad():\n",
    "    for slice_idx in range(155):\n",
    "        modality_dict = slice_dict[slice_idx]\n",
    "\n",
    "        x = torch.cat((modality_dict['flair'], \n",
    "                       modality_dict['t1'], \n",
    "                       modality_dict['t1ce'], \n",
    "                       modality_dict['t2']), dim=0)\n",
    "\n",
    "        # add batch dimension 1\n",
    "        x = x.unsqueeze(0)\n",
    "\n",
    "        # inference\n",
    "        x = x.to(device)\n",
    "        seg_pred_base = base_model(x)\n",
    "        seg_pred_base_aug = base_aug_model(x)\n",
    "        seg_pred_att = att_model(x)\n",
    "\n",
    "        # remove batch dimension 1\n",
    "\n",
    "        # append to list\n",
    "        seg_preds_base.append(seg_pred_base.cpu())\n",
    "        seg_preds_base_aug.append(seg_pred_base_aug.cpu())\n",
    "        seg_preds_att.append(seg_pred_att.cpu())\n",
    "        \n",
    "        # out = model(x)\n",
    "        # seg_preds.append(out)\n",
    "\n",
    "print(\"shape of seg_preds_base: \", len(seg_preds_base))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stack them into shape (155, 4, 240, 240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155\n"
     ]
    }
   ],
   "source": [
    "# vstack all slices\n",
    "seg_preds_base_list = torch.vstack(seg_preds_base)\n",
    "seg_preds_base_aug_list = torch.vstack(seg_preds_base_aug)\n",
    "seg_preds_att_list = torch.vstack(seg_preds_att)\n",
    "\n",
    "print(len(seg_preds_base))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detach from GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 240, 155)\n"
     ]
    }
   ],
   "source": [
    "# argmax an transpose\n",
    "seg_preds_base      = torch.argmax(seg_preds_base_list, dim=1).transpose(0, 2).cpu().detach().numpy()\n",
    "seg_preds_base_aug  = torch.argmax(seg_preds_base_aug_list, dim=1).transpose(0, 2).cpu().detach().numpy()\n",
    "seg_preds_att       = torch.argmax(seg_preds_att_list, dim=1).transpose(0, 2).cpu().detach().numpy()\n",
    "\n",
    "print (seg_preds_att.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize segmentation prediction vs groundtruth (overlay on top of Flair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 240, 155)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0164c30b840b4daeae96edf7575b2ac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=77, description='layer', max=154), Output()), _dom_classes=('widget-inteâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.visualize_3d_labels(layer)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_dict = {\n",
    "    0 : 'B/W = healthy',\n",
    "    1 : 'Red = necrotic',\n",
    "    2 : 'Green = edematous',\n",
    "    3 : 'Blue = enhancing'\n",
    "}\n",
    "print(np.shape(seg_target))\n",
    "\n",
    "# change colours of segmentation result  \n",
    "color_segmentation = np.zeros((240, 240, 155, 3), dtype=np.uint8)\n",
    "color_segmentation_pred_base = np.zeros((240, 240, 155, 3), dtype=np.uint8)\n",
    "color_segmentation_pred_base_aug = np.zeros((240, 240, 155, 3), dtype=np.uint8)\n",
    "color_segmentation_pred_att = np.zeros((240, 240, 155, 3), dtype=np.uint8)\n",
    "\n",
    "\n",
    "# replace 4 with 3\n",
    "seg_target[seg_target == 4] = 3\n",
    "\n",
    "# target                                             # Black (healthy tissue) = 0\n",
    "color_segmentation[seg_target == 1] = [255, 0, 0]    # Red (necrotic tumor core) = 1\n",
    "color_segmentation[seg_target == 2] = [0, 255, 0]    # Green (peritumoral edematous/invaded tissue) = 2\n",
    "color_segmentation[seg_target == 3] = [0, 0, 255]    # Blue (enhancing tumor) = 4\n",
    "\n",
    "# seg_preds_base                                                 # Black (healthy tissue) = 0\n",
    "color_segmentation_pred_base[seg_preds_base == 1] = [255, 0, 0]  # Red (necrotic tumor core) = 1\n",
    "color_segmentation_pred_base[seg_preds_base == 2] = [0, 255, 0]  # Green (peritumoral edematous/invaded tissue) = 2\n",
    "color_segmentation_pred_base[seg_preds_base == 3] = [0, 0, 255]  # Blue (enhancing tumor) = 4\n",
    "\n",
    "# seg_preds_base_aug                                                     # Black (healthy tissue) = 0\n",
    "color_segmentation_pred_base_aug[seg_preds_base_aug == 1] = [255, 0, 0]  # Red (necrotic tumor core) = 1\n",
    "color_segmentation_pred_base_aug[seg_preds_base_aug == 2] = [0, 255, 0]  # Green (peritumoral edematous/invaded tissue) = 2\n",
    "color_segmentation_pred_base_aug[seg_preds_base_aug == 3] = [0, 0, 255]  # Blue (enhancing tumor) = 4\n",
    "\n",
    "\n",
    "# seg_preds_att                                                # Black (healthy tissue) = 0\n",
    "color_segmentation_pred_att[seg_preds_att == 1] = [255, 0, 0]  # Red (necrotic tumor core) = 1\n",
    "color_segmentation_pred_att[seg_preds_att == 2] = [0, 255, 0]  # Green (peritumoral edematous/invaded tissue) = 2\n",
    "color_segmentation_pred_att[seg_preds_att == 3] = [0, 0, 255]  # Blue (enhancing tumor) = 4\n",
    "\n",
    "# alpha\n",
    "a = 0.3\n",
    "\n",
    "# text\n",
    "x_slice = 40\n",
    "\n",
    "def create_seg_figure(background, color_seg_tar, color_segmentation_pred_base_slice, color_segmentation_pred_base_aug_slice, color_segmentation_pred_att_slice, slice_idx):\n",
    "\n",
    "    plt.figure(figsize=(20, 20))\n",
    "\n",
    "    subtext = f\"BRaTS2021_{sample_id}\"\n",
    "    slice_txt = f\"{slice_idx:03d}\"\n",
    "\n",
    "    # target\n",
    "    plt.subplot(1, 4, 1)\n",
    "    plt.imshow(background, cmap='gray')\n",
    "    plt.imshow(color_seg_tar, cmap='bone', alpha=a)\n",
    "    plt.title(\"target\", fontsize=20)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.text(45, 230, subtext, fontsize=20, color='white')\n",
    "    plt.text(x_slice, 30, slice_txt, fontsize=30, color='white')\n",
    "\n",
    "    # U-net\n",
    "    plt.subplot(1, 4, 2)\n",
    "    plt.imshow(background, cmap='gray')\n",
    "    plt.imshow(color_segmentation_pred_base_slice, cmap='bone', alpha=a)\n",
    "    plt.title(\"U-net\", fontsize=20)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.text(45, 230, subtext, fontsize=20, color='white')\n",
    "    plt.text(x_slice, 30, slice_txt, fontsize=30, color='white')\n",
    "\n",
    "    # U-net + augmentations\n",
    "    plt.subplot(1, 4, 3)\n",
    "    plt.imshow(background, cmap='gray')\n",
    "    plt.imshow(color_segmentation_pred_base_aug_slice, cmap='bone', alpha=a)\n",
    "    plt.title(\"U-net+augmentations\", fontsize=20)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.text(45, 230, subtext, fontsize=20, color='white')\n",
    "    plt.text(x_slice, 30, slice_txt, fontsize=30, color='white')\n",
    "\n",
    "    # Attention U-net\n",
    "    plt.subplot(1, 4, 4)\n",
    "    plt.imshow(background, cmap='gray')\n",
    "    plt.imshow(color_segmentation_pred_att_slice, cmap='bone', alpha=a)\n",
    "    plt.title(\"Attention U-net\", fontsize=20)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.text(45, 230, subtext, fontsize=20, color='white') # case and slice id\n",
    "    plt.text(x_slice, 30, slice_txt, fontsize=30, color='white')\n",
    "\n",
    "\n",
    "    return plt\n",
    "\n",
    "\n",
    "def visualize_3d_labels(layer):\n",
    "    color_seg_tar = color_segmentation[:, :, layer, :]\n",
    "    color_segmentation_pred_base_slice = color_segmentation_pred_base[:, :, layer, :]\n",
    "    color_segmentation_pred_base_aug_slice = color_segmentation_pred_base_aug[:, :, layer, :]\n",
    "    color_segmentation_pred_att_slice = color_segmentation_pred_att[:, :, layer, :]\n",
    "\n",
    "    # print segmentation result\n",
    "    print([classes_dict[int(result)] for result in np.unique(seg_target[:, :, layer])])\n",
    " \n",
    "    background = T2[:, :, layer]\n",
    "    plot = create_seg_figure(background, color_seg_tar, color_segmentation_pred_base_slice, color_segmentation_pred_base_aug_slice, color_segmentation_pred_att_slice, layer)\n",
    "    plot.show()\n",
    "\n",
    "    return layer\n",
    "\n",
    "interact(visualize_3d_labels, layer=(0, Flair.shape[2] - 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('deepl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369441202de9f090a0e57b38848d3e186c8686d751df00b39c544f960532016f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
